{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "from pytorch_metric_learning import losses, miners, samplers, trainers, testers, distances\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "from cycler import cycler\n",
    "import record_keeper\n",
    "import pytorch_metric_learning\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch_metric_learning.utils.inference import MatchFinder, InferenceModel\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "from random import randint\n",
    "from os.path import expanduser\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri setini hazirlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shopee = pd.read_csv('/home/twoaday/research/data-sets/shopee/shopee-product-matching/train.csv')\n",
    "df_shopee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_file_list = []\n",
    "for parent_path, _, filenames in tqdm_notebook(os.walk(\n",
    "    '/home/twoaday/research/data-sets/stanford/Stanford_Online_Products')):\n",
    "    for f in filenames:\n",
    "        if '.JPG' in f or '.jpg' in f:\n",
    "            stanford_file_list.append(os.path.join(parent_path, f))\n",
    "stanford_file_list[0]\n",
    "\n",
    "df_stanford = []\n",
    "for f in tqdm_notebook(stanford_file_list):\n",
    "    label = f.split('/')[-1].split('_')[0]\n",
    "    df_stanford.append({'label':f'st_{label}', 'image_path': f})\n",
    "df = pd.DataFrame(df_stanford)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = pd.Categorical(df.label)\n",
    "df['label'] = df.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # layer_sizes[0] is the dimension of the input\n",
    "    # layer_sizes[-1] is the dimension of the output\n",
    "    def __init__(self, layer_sizes, final_relu=False):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        layer_sizes = [int(x) for x in layer_sizes]\n",
    "        num_layers = len(layer_sizes) - 1\n",
    "        final_relu_layer = num_layers if final_relu else num_layers - 1\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            input_size = layer_sizes[i]\n",
    "            curr_size = layer_sizes[i + 1]\n",
    "            if i < final_relu_layer:\n",
    "                layer_list.append(nn.ReLU(inplace=False))\n",
    "            layer_list.append(nn.Linear(input_size, curr_size))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "        self.last_linear = self.net[-1]\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        out = self.net(x) \n",
    "        return self._softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set trunk model and replace the softmax layer with an identity function\n",
    "trunk = torchvision.models.wide_resnet101_2(pretrained=True)\n",
    "trunk_output_size = trunk.fc.in_features\n",
    "trunk.fc = common_functions.Identity()\n",
    "trunk = torch.nn.DataParallel(trunk.to(device))\n",
    "embedding_size = 256\n",
    "# Set embedder model. This takes in the output of the trunk and outputs 64 dimensional embeddings\n",
    "embedder = torch.nn.DataParallel(MLP([trunk_output_size, embedding_size]).to(device))\n",
    "\n",
    "# Set optimizers\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "embedder_optimizer = torch.optim.Adam(embedder.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image transforms\n",
    "train_transform = transforms.Compose([transforms.Resize((64, 64)),\n",
    "                                    # transforms.RandomRotation(degrees=(0, 180)),\n",
    "                                    transforms.RandomHorizontalFlip(0.5),\n",
    "                                    # transforms.RandomVerticalFlip(0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize((64, 64)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veriyi train ve validation olarak ikiye ayiriyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = df.label.unique()\n",
    "\n",
    "val_set = np.random.choice(pids, 2000)\n",
    "df_test = df.loc[df.label.isin(val_set)]\n",
    "df_train = df.loc[~df.label.isin(val_set)]\n",
    "\n",
    "logging.info(f'Train size: {len(df_train)} Validation size: {len(df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veriyi modele beslemek icin data seti olusturuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductImageDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_set, train, transform):\n",
    "        self.data = data_set.image_path.values\n",
    "        self.targets = data_set.label.values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):            \n",
    "        if isinstance(index, slice):\n",
    "            img, target = self.data[index.start], self.targets[index.start]\n",
    "            img = Image.open(img).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img.unsqueeze(0).to(device)\n",
    "        else:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "            img = Image.open(img).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProductImageDataSet(df_train, True, train_transform)\n",
    "val_dataset = ProductImageDataSet(df_test,  False, val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function\n",
    "loss = losses.CircleLoss(m=0.4)\n",
    "\n",
    "# Set the mining function\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Set the dataloader sampler\n",
    "sampler = samplers.MPerClassSampler(train_dataset.targets, \n",
    "                                    m=2, \n",
    "                                    length_before_new_iter=len(train_dataset))\n",
    "\n",
    "# Set other training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "\n",
    "# Package the above stuff into dictionaries.\n",
    "models = {\"trunk\": trunk, \"embedder\": embedder}\n",
    "optimizers = {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}\n",
    "loss_funcs = {\"metric_loss\": loss}\n",
    "mining_funcs = {\"tuple_miner\": miner}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gorsellestirme ve loglama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'wide_resnet101_2_circle_softmax_{batch_size}_{embedding_size}'\n",
    "HOME = expanduser(\"~\")\n",
    "DL_MODELS_PATH = HOME + f'/trained_models/matching/images/{model_name}'\n",
    "\n",
    "record_keeper, _, _ = logging_presets.get_record_keeper(\"/tmp/\",\n",
    "                f\"/home/twoaday/tensorboard_logs/project_kusanagi/images/{model_name}\")\n",
    "hooks = logging_presets.get_hook_container(record_keeper)\n",
    "dataset_dict = {\"val\": val_dataset}\n",
    "model_folder = DL_MODELS_PATH\n",
    "\n",
    "def visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n",
    "    logging.info(\"UMAP plot for the {} split and label set {}\".format(split_name, keyname))\n",
    "    label_set = np.unique(labels)\n",
    "    num_classes = len(label_set)\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    plt.gca().set_prop_cycle(cycler(\"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]))\n",
    "    for i in range(num_classes):\n",
    "        idx = labels == label_set[i]\n",
    "        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)   \n",
    "    plt.show()\n",
    "\n",
    "# Create the tester\n",
    "tester = testers.GlobalEmbeddingSpaceTester(end_of_testing_hook = hooks.end_of_testing_hook, \n",
    "                                            visualizer = umap.UMAP(), \n",
    "                                            visualizer_hook = visualizer_hook,\n",
    "                                            dataloader_num_workers = 8,\n",
    "                                            accuracy_calculator=AccuracyCalculator(k=\"max_bin_count\"))\n",
    "\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(tester, \n",
    "                                            dataset_dict, \n",
    "                                            model_folder, \n",
    "                                            test_interval = 1,\n",
    "                                            patience = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.MetricLossOnly(models,\n",
    "                                optimizers,\n",
    "                                batch_size,\n",
    "                                loss_funcs,\n",
    "                                mining_funcs,\n",
    "                                train_dataset,\n",
    "                                sampler=sampler,\n",
    "                                dataloader_num_workers = 16,\n",
    "                                end_of_iteration_hook = hooks.end_of_iteration_hook,\n",
    "                                end_of_epoch_hook = end_of_epoch_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resimleri bastirmak icin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std]\n",
    ")\n",
    "\n",
    "def imshow(img, figsize=(8, 4)):\n",
    "    img = inv_normalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egittigimiz modeli yukluyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedVectorizer(nn.Module):\n",
    "    def __init__(self, trunk_model, embedder):\n",
    "        super(CombinedVectorizer, self).__init__()\n",
    "        self._trunk = trunk_model\n",
    "        self._embedder = embedder\n",
    "\n",
    "    def forward(self, title):\n",
    "        x = self._trunk(title)\n",
    "        return self._embedder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set trunk model and replace the softmax layer with an identity function\n",
    "trunk = torchvision.models.wide_resnet101_2(pretrained=True)\n",
    "trunk_output_size = trunk.fc.in_features\n",
    "trunk.fc = common_functions.Identity()\n",
    "embedding_size = 256\n",
    "\n",
    "# Set embedder model. This takes in the output of the trunk and outputs 64 dimensional embeddings\n",
    "embedder = MLP([trunk_output_size, embedding_size]).to(device)\n",
    "trunk_path = f'/home/twoaday/trained_models/matching/images/{model_name}/trunk_11.pth'\n",
    "embedder_path = f'/home/twoaday/trained_models/matching/images/{model_name}/embedder_11.pth'\n",
    "trunk.load_state_dict(torch.load(trunk_path, map_location=device))\n",
    "embedder.load_state_dict(torch.load(embedder_path, map_location=device))\n",
    "\n",
    "model = CombinedVectorizer(trunk, embedder).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resim arama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=mean, std=std)])\n",
    "labels_to_indices = c_f.get_labels_to_indices(val_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_finder = MatchFinder(distance=CosineSimilarity(), threshold=0.7)\n",
    "inference_model = InferenceModel(model, match_finder=match_finder, batch_size=1)\n",
    "inference_model.train_indexer(val_dataset, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_no = randint(0, len(val_dataset))\n",
    "img = val_dataset[index_no][0].unsqueeze(0)\n",
    "print(\"query image\")\n",
    "imshow(torchvision.utils.make_grid(img))\n",
    "indices, distances = inference_model.get_nearest_neighbors(img.to(device), k=10)\n",
    "nearest_imgs = [val_dataset[i][0] for i in indices[0]]\n",
    "print(\"nearest images\")\n",
    "imshow(torchvision.utils.make_grid(nearest_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
